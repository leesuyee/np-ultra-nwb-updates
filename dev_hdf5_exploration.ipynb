{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the recording session summary table \n",
    "recording_summary = \"/Volumes/scratch/andrew.shelton/NPUltra_data/raw_npultra_data/NPUltra_recording_summary.xlsx\"\n",
    "recording_summary_table = pd.read_excel(recording_summary)\n",
    "\n",
    "# Filter table for sessions of interest  \n",
    "filtered_sessions = recording_summary_table[\n",
    "    (recording_summary_table['experiment'] == 'NPUltra_psychedelics') &\n",
    "    (recording_summary_table['uploaded to CO'] == 'yes')]\n",
    "\n",
    "filtered_sessions.head()\n",
    "\n",
    "session_list = filtered_sessions['session'].tolist()\n",
    "session_list = session_list[:1] # For testing, only take the first session \n",
    "\n",
    "for session in range(len(session_list)):\n",
    "    base_path = \"/Volumes/scratch/andrew.shelton/NPUltra_data/raw_npultra_data/\"\n",
    "    session_folder = f\"{base_path}{session_list[session]}\"\n",
    "\n",
    "    # Navigate to the processed_data/units/ folder within the session folder \n",
    "    if os.path.exists(session_folder):\n",
    "        units_table_path = os.path.join(session_folder, \"behavior\")\n",
    "        # Load hdf5 with this naming scheme: /Volumes/aind/scratch/andrew.shelton/NPUltra_data/raw_npultra_data/2024-05-14_714527/behavior/OptoTagging_714527_20240514_110641.hdf5\n",
    "        hdf5_files = [f for f in os.listdir(units_table_path) if f.endswith('.hdf5') and f.startswith('OptoTagging_')]\n",
    "        if hdf5_files:\n",
    "            hdf5_file = hdf5_files[0]\n",
    "            hdf5_file_path = os.path.join(units_table_path, hdf5_file)\n",
    "            # Load the hdf5 file\n",
    "            # What do I need to install to use read_hdf? \n",
    "\n",
    "            # Why do I need a key? \n",
    "            \n",
    "            stim_parameters = pd.read_hdf(hdf5_file_path)\n",
    "\n",
    "            # # Convert the DataFrame to a dictionary\n",
    "            # stim_parameters_dict = stim_parameters.to_dict(orient='records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hdf5_file_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m group_info = {}\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Open and inspect the HDF5 file\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m h5py.File(\u001b[43mhdf5_file_path\u001b[49m, \u001b[33m'\u001b[39m\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m     11\u001b[39m     root_keys = \u001b[38;5;28mlist\u001b[39m(f.keys())\n\u001b[32m     12\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mRoot level keys:\u001b[39m\u001b[33m\"\u001b[39m, root_keys)\n",
      "\u001b[31mNameError\u001b[39m: name 'hdf5_file_path' is not defined"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "# Initialize lists to store keys and structure info\n",
    "all_keys = []\n",
    "dataset_info = {}\n",
    "group_info = {}\n",
    "\n",
    "# Open and inspect the HDF5 file\n",
    "with h5py.File(hdf5_file_path, 'r') as f:\n",
    "    root_keys = list(f.keys())\n",
    "    print(\"Root level keys:\", root_keys)\n",
    "    \n",
    "    # Function to recursively explore and store structure\n",
    "    def explore_group(group, level=0, parent_path=\"\"):\n",
    "        indent = \"  \" * level\n",
    "        for key in group.keys():\n",
    "            full_path = f\"{parent_path}/{key}\" if parent_path else key\n",
    "            all_keys.append(full_path)\n",
    "            \n",
    "            item = group[key]\n",
    "            if isinstance(item, h5py.Group):\n",
    "                print(f\"{indent}{key}/ (Group)\")\n",
    "                group_info[full_path] = {\"type\": \"group\", \"level\": level}\n",
    "                explore_group(item, level + 1, full_path)\n",
    "            elif isinstance(item, h5py.Dataset):\n",
    "                print(f\"{indent}{key} (Dataset): shape={item.shape}, dtype={item.dtype}\")\n",
    "                dataset_info[full_path] = {\n",
    "                    \"type\": \"dataset\", \n",
    "                    \"shape\": item.shape, \n",
    "                    \"dtype\": str(item.dtype),\n",
    "                    \"level\": level\n",
    "                }\n",
    "    \n",
    "    explore_group(f)\n",
    "\n",
    "# Now you have the keys stored in variables:\n",
    "print(f\"\\nTotal keys found: {len(all_keys)}\")\n",
    "print(f\"Dataset keys: {list(dataset_info.keys())}\")\n",
    "print(f\"Group keys: {list(group_info.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access a specific dataset by key\n",
    "dataset_key = \"optoInterval\"  # Replace with actual key from dataset_info.keys()\n",
    "\n",
    "with h5py.File(hdf5_file_path, 'r') as f:\n",
    "    if dataset_key in dataset_info:\n",
    "        dataset = f[dataset_key]\n",
    "        \n",
    "        # Display dataset information\n",
    "        print(f\"Dataset: {dataset_key}\")\n",
    "        print(f\"Shape: {dataset.shape}\")\n",
    "        print(f\"Data type: {dataset.dtype}\")\n",
    "        print(f\"Attributes: {dict(dataset.attrs)}\")\n",
    "        \n",
    "        # Read the actual data\n",
    "        data = dataset[:]\n",
    "        print(f\"Data preview:\\n{data}\")\n",
    "        \n",
    "        # If you want to convert to pandas DataFrame (for 2D data)\n",
    "        if len(dataset.shape) <= 2:\n",
    "            import pandas as pd\n",
    "            df = pd.DataFrame(data)\n",
    "            print(f\"As DataFrame:\\n{df.head()}\")\n",
    "    else:\n",
    "        print(f\"Key '{dataset_key}' not found in datasets\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
