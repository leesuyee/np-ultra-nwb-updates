{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import hdmf_zarr\n",
    "from hdmf.common import DynamicTable\n",
    "import numpy as np\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "import glob \n",
    "\n",
    "def get_sessions_of_interest(summary_path: str, experiment_filter: str = 'NPUltra_psychedelics', \n",
    "                           upload_filter: str = 'yes') -> List[str]:\n",
    "    \"\"\"\n",
    "    Generate list of sessions that meet specified criteria from recording summary.\n",
    "    \n",
    "    Args:\n",
    "        summary_path: Path to NPUltra_recording_summary.xlsx\n",
    "        experiment_filter: Experiment type to filter for\n",
    "        upload_filter: Code Ocean Upload status to filter for\n",
    "    \n",
    "    Returns:\n",
    "        List of sessions that match the criteria \n",
    "    \"\"\"\n",
    "    recording_summary_table = pd.read_excel(summary_path)\n",
    "    \n",
    "    filtered_sessions = recording_summary_table[\n",
    "        (recording_summary_table['experiment'] == experiment_filter) &\n",
    "        (recording_summary_table['uploaded to CO'] == upload_filter)\n",
    "    ]\n",
    "    \n",
    "    session_list = filtered_sessions['session'].tolist()\n",
    "    print(f\"Found {len(session_list)} sessions matching criteria\")\n",
    "    \n",
    "    return session_list\n",
    "\n",
    "def extract_analysis_table(session_folder: str) -> Optional[pd.DataFrame]:\n",
    "    # Optional indicates that function can return None or pd.DataFrame \n",
    "    \"\"\"\n",
    "    Extract [postprocessed] units table from the session folder. This table should be renamed to analysis_table to avoid \n",
    "    confusion with the existing units table in the NWB file. \n",
    "    \n",
    "    Args:\n",
    "        session_folder: Path to session directory\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with analysis data or None if not found\n",
    "    \"\"\"\n",
    "    units_table_path = os.path.join(session_folder, \"processed_data\", \"units\")\n",
    "    \n",
    "    if not os.path.exists(units_table_path):\n",
    "        print(f\"Units folder does not exist in {session_folder}\")\n",
    "        return None\n",
    "    \n",
    "    # Find units table pickle (exclude units_epochs)\n",
    "    units_files = [f for f in os.listdir(units_table_path) \n",
    "                   if 'units_epoch' in f and 'units_epochs' not in f]\n",
    "    \n",
    "    if not units_files:\n",
    "        print(f\"No units table files found in {units_table_path}\")\n",
    "        return None\n",
    "    \n",
    "    units_table_file = os.path.join(units_table_path, units_files[0])\n",
    "    analysis_table = pd.read_pickle(units_table_file)\n",
    "    print(f\"Loaded units table from {units_table_file}\")\n",
    "    \n",
    "    return analysis_table\n",
    "\n",
    "def extract_stimulus_table(session_folder: str) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Extract stimulus table from session folder and add stimulus information from metadata.\n",
    "    \n",
    "    Args:\n",
    "        session_folder: Path to session directory\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with stimulus data or None if not found\n",
    "    \"\"\"\n",
    "    stim_path = os.path.join(session_folder, \"processed_data\", \"stim\")\n",
    "    \n",
    "    if not os.path.exists(stim_path):\n",
    "        print(f\"Stimulus folder does not exist in {session_folder}\")\n",
    "        return None\n",
    "    \n",
    "    # Initialize stimulus table\n",
    "    stim_table = pd.DataFrame(columns=['start_time', 'stop_time', 'power', 'stim_name', \n",
    "                                     'emission_location', 'wavelength'])\n",
    "    \n",
    "    # Load stimulus table from opto_stim_df.csv\n",
    "    stim_table_path = os.path.join(stim_path, \"opto_stim_df.csv\")\n",
    "    if not os.path.exists(stim_table_path):\n",
    "        print(f\"No stimulus table files found in {stim_path}\")\n",
    "        return None\n",
    "    \n",
    "    opto_stim_df = pd.read_csv(stim_table_path)\n",
    "    print(f\"Loaded stimulus table from {stim_table_path}\")\n",
    "    \n",
    "    # Populate stimulus table\n",
    "    stim_table['start_time'] = opto_stim_df['stim_on']\n",
    "    stim_table['stop_time'] = opto_stim_df['stim_off']\n",
    "    stim_table['stim_name'] = opto_stim_df['epoch_label']\n",
    "    stim_table['emission_location'] = opto_stim_df['probe']\n",
    "    \n",
    "    # Extract more stimulus information from session.json metadata \n",
    "    metadata_path = os.path.join(session_folder, \"metadata\", \"session.json\")\n",
    "    if os.path.exists(metadata_path):\n",
    "        with open(metadata_path, 'r') as f:\n",
    "            metadata = json.load(f)\n",
    "            \n",
    "        # Extract wavelength and power from first stimulus epoch light_source_config \n",
    "        wavelength = None\n",
    "        power = None\n",
    "        for epoch in metadata.get('stimulus_epochs', []):\n",
    "            light_config = epoch.get('light_source_config')\n",
    "            if light_config is not None:\n",
    "                wavelength = light_config.get('wavelength')\n",
    "                power = light_config.get('laser_power')\n",
    "                break # Only store first epoch's config as the rest are redundant \n",
    "        \n",
    "        stim_table['wavelength'] = wavelength\n",
    "        stim_table['power'] = power\n",
    "    \n",
    "    return stim_table\n",
    "\n",
    "def create_epoch_table(session_folder: str) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Create epoch table from stimulus epoch files and metadata.\n",
    "    \n",
    "    Args:\n",
    "        session_folder: Path to session directory\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with epoch information\n",
    "    \"\"\"\n",
    "    epoch_path = os.path.join(session_folder, \"processed_data\", \"stim\")\n",
    "    \n",
    "    if not os.path.exists(epoch_path):\n",
    "        print(f\"Stimulus folder does not exist in {session_folder}\")\n",
    "        return None\n",
    "    \n",
    "    # Process each CSV file that contains trial by trial timing information for each stimulus epoch\n",
    "    all_epoch_data = []\n",
    "    epoch_files = [f for f in os.listdir(epoch_path) if f.endswith('.csv')]\n",
    "    \n",
    "    if not epoch_files:\n",
    "        print(f\"No CSV files found in {epoch_path}\")\n",
    "        return None\n",
    "    \n",
    "    # Each CSV corresponds to a different stimulus epoch type \n",
    "    for epoch_file in epoch_files:\n",
    "        file_path = os.path.join(epoch_path, epoch_file)\n",
    "        \n",
    "        # Read required columns, handle missing stim_off \n",
    "        # Only the OptoTagging epochs have a stim_off column, others do not \n",
    "        try:\n",
    "            temp_df = pd.read_csv(file_path, usecols=['stim_on', 'stim_off', 'epoch_label'])\n",
    "        except ValueError:\n",
    "            temp_df = pd.read_csv(file_path, usecols=['stim_on', 'epoch_label'])\n",
    "            temp_df['stim_off'] = pd.NA\n",
    "        \n",
    "        all_epoch_data.append(temp_df)\n",
    "    \n",
    "    # Combine trial-by-trial information across all stimulus epochs\n",
    "    combined_df = pd.concat(all_epoch_data, ignore_index=True)\n",
    "    \n",
    "    # Group combined dataframe by epoch_label to get start/stop times for the entire epoch \n",
    "    epoch_summary = combined_df.groupby('epoch_label').apply(\n",
    "        lambda group: pd.Series({\n",
    "            'start_time': group['stim_on'].min(),\n",
    "            'stop_time': group['stim_off'].max() if 'OptoTagging' in group.name else group['stim_on'].max()\n",
    "        }) \n",
    "    ).reset_index() # Use max for stim_off only if it exists, otherwise use max of stim_on\n",
    "    \n",
    "    # Rename columns \n",
    "    epoch_table = epoch_summary.rename(columns={'epoch_label': 'stim_name'})\n",
    "    \n",
    "    # Sort epochs by start_time \n",
    "    epoch_table = epoch_table.sort_values(by='start_time').reset_index(drop=True)\n",
    "    \n",
    "    return epoch_table\n",
    "\n",
    "def add_injection_epoch(epoch_table: pd.DataFrame, session_folder: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Add injection epoch between OptoTagging_0 and Spontaneous_1. \n",
    "    \n",
    "    Args:\n",
    "        epoch_table: Existing epoch table\n",
    "        session_folder: Path to session directory\n",
    "    \n",
    "    Returns:\n",
    "        Modified epoch table with injection epoch added\n",
    "    \"\"\"\n",
    "    metadata_path = os.path.join(session_folder, \"metadata\", \"session.json\")\n",
    "    \n",
    "    if not os.path.exists(metadata_path):\n",
    "        print(f\"Metadata file does not exist in {metadata_path}\")\n",
    "        return epoch_table\n",
    "    \n",
    "    with open(metadata_path, 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "    \n",
    "    # Determine injection type from OptoTagging stimulus epoch note \n",
    "    injection_name = \"Injection\"  # Default if missing info in metadata \n",
    "    for epoch in metadata.get('stimulus_epochs', []):\n",
    "        if epoch.get('stimulus_name') == 'OptoTagging':\n",
    "            notes = epoch.get('notes', '')\n",
    "            if 'Saline' in notes:\n",
    "                injection_name = \"Saline_Injection\"\n",
    "            elif 'Psilocybin' in notes:\n",
    "                injection_name = \"Psilocybin_Injection\"\n",
    "            break\n",
    "    \n",
    "    # Get start and stop times for new epoch based on the timing of Optotaggin_0 and Spontaneous_1 epochs \n",
    "    opto_0_stop = None\n",
    "    spont_1_start = None\n",
    "    \n",
    "    for idx, row in epoch_table.iterrows():\n",
    "        if row['stim_name'] == 'OptoTagging_0': # Get value of stop_time for OptoTagging_0\n",
    "            opto_0_stop = row['stop_time']\n",
    "        elif row['stim_name'] == 'Spontaneous_1': # Get value of start_time for Spontaneous_1 \n",
    "            spont_1_start = row['start_time']\n",
    "    \n",
    "    if opto_0_stop is not None and spont_1_start is not None:\n",
    "        # Create injection epoch with time buffer \n",
    "        injection_start = opto_0_stop + 0.01\n",
    "        injection_end = spont_1_start - 0.01\n",
    "        \n",
    "        # Identify the correct index in epoch_table to insert the new epoch (after Optotagging_0)\n",
    "        insert_idx = None\n",
    "        for idx, row in epoch_table.iterrows():\n",
    "            if row['stim_name'] == 'OptoTagging_0':\n",
    "                insert_idx = idx + 1\n",
    "                break\n",
    "        \n",
    "        # Fill out the new row with injection information \n",
    "        new_row = pd.DataFrame({\n",
    "            'stim_name': [injection_name],\n",
    "            'start_time': [injection_start],\n",
    "            'stop_time': [injection_end]\n",
    "        })\n",
    "        \n",
    "        # Insert the new row at the correct index into the epoch_table \n",
    "        if insert_idx is not None:\n",
    "            epoch_table = pd.concat([\n",
    "                epoch_table.iloc[:insert_idx], # Keep rows before the insertion point \n",
    "                new_row, \n",
    "                epoch_table.iloc[insert_idx:] # Keep rows after the insertion point \n",
    "            ], ignore_index=True)\n",
    "            print(f\"Added {injection_name} epoch from {injection_start} to {injection_end}\")\n",
    "    \n",
    "    return epoch_table\n",
    "\n",
    "def convert_dataframe_to_dynamic_table(df: pd.DataFrame, table_name: str) -> DynamicTable:\n",
    "    \"\"\"\n",
    "    Convert pandas DataFrame to DynamicTable object compatible with NWB files. \n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        table_name: Name for the dynamic table\n",
    "    \n",
    "    Returns:\n",
    "        DynamicTable object\n",
    "    \"\"\"\n",
    "    dynamic_table = DynamicTable.from_dataframe(\n",
    "        name=table_name.lower().replace(' ', '_'),\n",
    "        df=df\n",
    "    )\n",
    "    # name ensures the input is lowercase and has underscores  \n",
    "    print(f\"Created DynamicTable '{table_name}' with {len(df)} rows and {len(df.columns)} columns\")\n",
    "    return dynamic_table\n",
    "\n",
    "def modify_nwb_file(original_path: str, new_path: str, analysis_table: Optional[pd.DataFrame] = None,\n",
    "                   stim_table: Optional[pd.DataFrame] = None, epoch_table: Optional[pd.DataFrame] = None) -> None:\n",
    "    \"\"\"\n",
    "    Modify existing NWB file with new tables and save to new location. \n",
    "    Tables are pandas DataFrames that will be converted to DynamicTables. \n",
    "    \n",
    "    Args:\n",
    "        original_path: Path to input NWB file\n",
    "        new_path: Path for output NWB file\n",
    "        analysis_table: Analysis table to add\n",
    "        stim_table: Stimulus table to add\n",
    "        epoch_table: Epoch table to add\n",
    "    \"\"\"\n",
    "    # Create directory for new file\n",
    "    os.makedirs(os.path.dirname(new_path), exist_ok=True)\n",
    "    \n",
    "    # Open existing NWB file\n",
    "    io = hdmf_zarr.NWBZarrIO(original_path, mode=\"r\")\n",
    "    nwbfile = io.read()\n",
    "    \n",
    "    # Add analysis table\n",
    "    if analysis_table is not None:\n",
    "        print(\"Processing analysis table...\")\n",
    "        try:\n",
    "            analysis_dynamic_table = convert_dataframe_to_dynamic_table(\n",
    "                df=analysis_table,\n",
    "                table_name=\"analysis_table\"\n",
    "            )\n",
    "            nwbfile.add_analysis(analysis_dynamic_table)\n",
    "            print(f\"Added analysis table with {len(analysis_table)} rows\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to add analysis table: {e}\")\n",
    "            raise\n",
    "    \n",
    "    # Add stimulus table\n",
    "    if stim_table is not None:\n",
    "        print(\"Processing stimulus table...\")\n",
    "        try:\n",
    "            stimulus_dynamic_table = convert_dataframe_to_dynamic_table(\n",
    "                df=stim_table,\n",
    "                table_name=\"stimulus_table\"\n",
    "            )\n",
    "            nwbfile.add_stimulus(stimulus_dynamic_table)\n",
    "            print(f\"Added stimulus table with {len(stim_table)} rows\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to add stimulus table: {e}\")\n",
    "            raise\n",
    "    \n",
    "    # Add epoch table\n",
    "    if epoch_table is not None:\n",
    "        print(\"Processing epoch table...\")\n",
    "        try: # Constructs epoch table row by row from DataFrame  \n",
    "            for idx, row in epoch_table.iterrows():\n",
    "                tags = [str(row['stim_name'])] if 'stim_name' in row and pd.notna(row['stim_name']) else []\n",
    "                nwbfile.add_epoch(\n",
    "                    start_time=float(row['start_time']),\n",
    "                    stop_time=float(row['stop_time']),\n",
    "                    tags=tags\n",
    "                )\n",
    "            print(f\"Added {len(epoch_table)} epochs\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to add epoch table: {e}\")\n",
    "            raise\n",
    "    \n",
    "    # Write modified NWB to new file \n",
    "    with hdmf_zarr.NWBZarrIO(new_path, mode='w') as export_io:\n",
    "        export_io.export(src_io=io, nwbfile=nwbfile)\n",
    "    \n",
    "    io.close()\n",
    "    print(f\"Saved to: {new_path}\")\n",
    "\n",
    "def process_single_session(session_name: str, base_path: str, output_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Process a single session through the complete pipeline. Finds correct file paths based on session_name for all necessary directories and files. \n",
    "    \n",
    "    Args:\n",
    "        session_name: Name of the session to process\n",
    "        base_path: Base path containing session folders\n",
    "        output_path: Output directory for modified NWB files\n",
    "    \"\"\"\n",
    "    session_folder = os.path.join(base_path, session_name)\n",
    "    print(f\"\\nProcessing session: {session_name}\")\n",
    "    \n",
    "    if not os.path.exists(session_folder):\n",
    "        print(f\"Session folder {session_folder} does not exist\")\n",
    "        return\n",
    "    \n",
    "    # Extract all tables\n",
    "    analysis_table = extract_analysis_table(session_folder)\n",
    "    stim_table = extract_stimulus_table(session_folder)\n",
    "    epoch_table = create_epoch_table(session_folder)\n",
    "    \n",
    "    # Add injection epoch if epoch table exists\n",
    "    if epoch_table is not None:\n",
    "        epoch_table = add_injection_epoch(epoch_table, session_folder)\n",
    "    \n",
    "    # Find original NWB file\n",
    "    nwb_search_path = os.path.join(session_folder, f\"*{session_name.split('_')[1]}*\")\n",
    "    \n",
    "    # First, find directories containing the session name\n",
    "    cleaned_session_name = session_name[0].replace('-', '')\n",
    "    # Within the session folder, search for directories that contain the cleaned session name\n",
    "    session_dirs = glob.glob(os.path.join(session_folder, f\"*{cleaned_session_name}*\")) \n",
    "    if not session_dirs:\n",
    "        print(f\"No directory containing '{cleaned_session_name}' found in {session_folder}\")\n",
    "\n",
    "    # Then search for NWB files containing the session name within those directories\n",
    "    nwb_files = []\n",
    "    for session_dir in session_dirs:\n",
    "        # instead, search for a pattern that ends in .nwb \n",
    "        nwb_pattern = os.path.join(session_dir, f\"*.nwb\")\n",
    "        # Get full file path to nwb file \n",
    "        nwb_files.extend(glob.glob(nwb_pattern))\n",
    "        # Get nwb file name to match for output file \n",
    "        nwb_files.extend([f for f in os.listdir(session_dir) if f.endswith('.nwb')])\n",
    "\n",
    "    if not nwb_files:\n",
    "        print(f\"No NWB file containing '{session_name}' found in session directories\")\n",
    "\n",
    "    original_nwb = nwb_files[0]  # Take the first match\n",
    "    print(f\"Found NWB file: {original_nwb}\")\n",
    "    new_nwb = os.path.join(output_path, nwb_files[1])\n",
    "    \n",
    "    # Modify NWB file\n",
    "    modify_nwb_file(\n",
    "        original_path=original_nwb,\n",
    "        new_path=new_nwb,\n",
    "        analysis_table=analysis_table,\n",
    "        stim_table=stim_table,\n",
    "        epoch_table=epoch_table\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12 sessions matching criteria\n",
      "\n",
      "Processing session: 2024-05-16_714789\n",
      "Loaded units table from /Volumes/scratch/andrew.shelton/NPUltra_data/raw_npultra_data/2024-05-16_714789/processed_data/units/2024-05-16_714789_1_units_epoch.pkl\n",
      "Loaded stimulus table from /Volumes/scratch/andrew.shelton/NPUltra_data/raw_npultra_data/2024-05-16_714789/processed_data/stim/opto_stim_df.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1f/h0zlr4c90tg4sf8rv_22d7v80000gp/T/ipykernel_82879/762740619.py:162: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  combined_df = pd.concat(all_epoch_data, ignore_index=True)\n",
      "/var/folders/1f/h0zlr4c90tg4sf8rv_22d7v80000gp/T/ipykernel_82879/762740619.py:165: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  epoch_summary = combined_df.groupby('epoch_label').apply(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added Saline_Injection epoch from 2719.7499900000003 to 2781.6479499999996\n",
      "Found NWB file: /Volumes/scratch/andrew.shelton/NPUltra_data/raw_npultra_data/2024-05-16_714789/20240516_714789_1/ecephys_714789_2024-05-16_13-16-59_experiment1_recording1.nwb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/suyee.lee/np-ultra-psychedelics/.venv/lib/python3.12/site-packages/hdmf/spec/namespace.py:535: UserWarning: Ignoring cached namespace 'core' version 2.7.0 because version 2.8.0 is already loaded.\n",
      "  warn(\"Ignoring cached namespace '%s' version %s because version %s is already loaded.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing analysis table...\n",
      "Created DynamicTable 'analysis_table' with 910 rows and 54 columns\n",
      "Added analysis table with 910 rows\n",
      "Processing stimulus table...\n",
      "Created DynamicTable 'stimulus_table' with 600 rows and 6 columns\n",
      "Added stimulus table with 600 rows\n",
      "Processing epoch table...\n",
      "Added 13 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/suyee.lee/np-ultra-psychedelics/.venv/lib/python3.12/site-packages/zarr/storage.py:455: FutureWarning: missing object_codec for object array; this will raise a ValueError in version 3.0\n",
      "  _init_array_metadata(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to: /Volumes/scratch/suyee.lee/ecephys_714789_2024-05-16_13-16-59_experiment1_recording1.nwb\n"
     ]
    }
   ],
   "source": [
    "# Example usage: \n",
    "filtered_session_list = get_sessions_of_interest(\n",
    "    summary_path = \"/Volumes/scratch/andrew.shelton/NPUltra_data/raw_npultra_data/NPUltra_recording_summary.xlsx\",\n",
    "    experiment_filter = \"NPUltra_psychedelics\", \n",
    "    upload_filter = 'yes'\n",
    "    )\n",
    "\n",
    "process_single_session(\n",
    "    session_name = filtered_session_list[2],  # Process one session as an example \n",
    "    base_path = ' /Volumes/scratch/andrew.shelton/NPUltra_data/raw_npultra_data/',\n",
    "    output_path = '/Volumes/scratch/suyee.lee'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded new NWB file: /Volumes/scratch/suyee.lee/ecephys_714789_2024-05-16_13-16-59_experiment1_recording1.nwb\n",
      "Processing modules: ['ecephys']\n",
      "Stimulus presentations and epochs: ['stimulus_table']\n",
      "Epochs: epochs pynwb.epoch.TimeIntervals at 0x17964502048\n",
      "Fields:\n",
      "  colnames: ['start_time' 'stop_time' 'tags']\n",
      "  columns: (\n",
      "    start_time <class 'hdmf.common.table.VectorData'>,\n",
      "    stop_time <class 'hdmf.common.table.VectorData'>,\n",
      "    tags_index <class 'hdmf.common.table.VectorIndex'>,\n",
      "    tags <class 'hdmf.common.table.VectorData'>\n",
      "  )\n",
      "  description: experimental epochs\n",
      "  id: id <class 'hdmf.common.table.ElementIdentifiers'>\n",
      "\n",
      "Trials: None\n"
     ]
    }
   ],
   "source": [
    "# Check outputs of new nwb file \n",
    "# \n",
    "# new_nwb = '/Volumes/scratch/suyee.lee/ecephys_714789_2024-05-16_13-16-59_experiment1_recording1.nwb'\n",
    "\n",
    "with hdmf_zarr.NWBZarrIO(new_nwb, mode='r') as io:\n",
    "    new_nwbfile = io.read()\n",
    "    print(\"Loaded new NWB file:\", new_nwb)\n",
    "    print(\"Processing modules:\", list(new_nwbfile.processing.keys()))\n",
    "    print(\"Stimulus presentations and epochs:\", list(new_nwbfile.stimulus.keys()) if hasattr(new_nwbfile, 'stimulus') else \"None\")\n",
    "    print(\"Epochs:\", new_nwbfile.epochs)  # Should show the added epochs\n",
    "    print(\"Trials:\", new_nwbfile.trials)  # Should show the added trials\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
