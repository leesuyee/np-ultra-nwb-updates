{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re \n",
    "import json\n",
    "import hdmf_zarr\n",
    "from hdmf.common import DynamicTable\n",
    "import numpy as np\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "import glob \n",
    "\n",
    "def get_sessions_of_interest(summary_path: str, experiment_filter: str = 'NPUltra_psychedelics', \n",
    "                           upload_filter: str = 'yes') -> List[str]:\n",
    "    \"\"\"\n",
    "    Generate list of sessions that meet specified criteria from recording summary.\n",
    "    \n",
    "    Args:\n",
    "        summary_path: Path to NPUltra_recording_summary.xlsx\n",
    "        experiment_filter: Experiment type to filter for\n",
    "        upload_filter: Code Ocean Upload status to filter for\n",
    "    \n",
    "    Returns:\n",
    "        List of sessions that match the criteria \n",
    "    \"\"\"\n",
    "    recording_summary_table = pd.read_excel(summary_path)\n",
    "    \n",
    "    filtered_sessions = recording_summary_table[\n",
    "        (recording_summary_table['experiment'] == experiment_filter) &\n",
    "        (recording_summary_table['uploaded to CO'] == upload_filter)\n",
    "    ]\n",
    "    \n",
    "    session_list = filtered_sessions['session'].tolist()\n",
    "    print(f\"Found {len(session_list)} sessions matching criteria\")\n",
    "    \n",
    "    return session_list\n",
    "\n",
    "def extract_units_table(session_name, session_folder: str) -> Optional[pd.DataFrame]:\n",
    "    \"\"\" Get units table from existing NWB file in session folder and modify table names and organization. \n",
    "    Args: \n",
    "        session_name: Name of the session\n",
    "        session_folder: Path to session directory\n",
    "\n",
    "    Returns: \n",
    "        DataFrame with modified units table or None if not found\n",
    "    \"\"\"\n",
    "    # Parse the session name to get the subject ID (YYYY-MM-DD_XXXXXX format) \n",
    "    session_pattern = r'\\d{4}-\\d{2}-\\d{2}_(\\d{6})' \n",
    "    match = re.search(session_pattern, session_name)\n",
    "    subject_id = match.group(1) \n",
    "    nwb_path = [] \n",
    "\n",
    "    # Search for directories in main session folder that contain the subject ID \n",
    "    subject_dir = glob.glob(os.path.join(session_folder, f\"*{subject_id}*\"))   \n",
    "\n",
    "    if not subject_dir:\n",
    "        print(f\"No directory containing '{subject_id}' found in {session_folder}\")\n",
    "\n",
    "    # Get the path to the NWB directory \n",
    "    nwb_pattern = os.path.join(subject_dir[0], \"*.nwb\")\n",
    "    nwb_path.extend(glob.glob(nwb_pattern)) \n",
    "\n",
    "    if not os.path.exists(nwb_path[0]):\n",
    "        print(f\"NWB file {nwb_path[0]} does not exist\")\n",
    "\n",
    "    # Load NWB file using NWBZarrIO, load units table \n",
    "    with hdmf_zarr.NWBZarrIO(nwb_path[0], mode='r') as io:\n",
    "        nwbfile = io.read()\n",
    "        # Get units table \n",
    "        units_table = nwbfile.units.to_dataframe()\n",
    "\n",
    "    # Modify units table \n",
    "    modified_units_table = units_table.copy() \n",
    "    modified_units_table = modified_units_table.rename(columns={\n",
    "        'ks_unit_id': 'unitID'\n",
    "    })\n",
    "\n",
    "    # Reorganize columns alphabetically \n",
    "    modified_units_table = modified_units_table.reindex(sorted(modified_units_table.columns), axis=1)\n",
    "    # Put unitID at the beginning of the table  \n",
    "    unitID_col = modified_units_table.pop('unitID') # Remove the column \n",
    "    modified_units_table.insert(0, 'unitID', unitID_col) # Insert it back at the beginning \n",
    "\n",
    "    # Save modified units table \n",
    "    return modified_units_table \n",
    "\n",
    "def extract_analysis_table(session_folder: str) -> Optional[pd.DataFrame]:\n",
    "    # Optional indicates that function can return None or pd.DataFrame \n",
    "    \"\"\"\n",
    "    Extract [postprocessed] units table from the session folder. This table should be renamed to analysis_table to avoid \n",
    "    confusion with the existing units table in the NWB file. \n",
    "    \n",
    "    Args:\n",
    "        session_folder: Path to session directory\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with analysis data or None if not found\n",
    "    \"\"\"\n",
    "    units_table_path = os.path.join(session_folder, \"processed_data\", \"units\")\n",
    "    \n",
    "    if not os.path.exists(units_table_path):\n",
    "        print(f\"Units folder does not exist in {session_folder}\")\n",
    "        return None\n",
    "    \n",
    "    # Find units table pickle (exclude units_epochs)\n",
    "    units_files = [f for f in os.listdir(units_table_path) \n",
    "                   if 'units_epoch' in f and 'units_epochs' not in f]\n",
    "    \n",
    "    if not units_files:\n",
    "        print(f\"No units table files found in {units_table_path}\")\n",
    "        return None\n",
    "    \n",
    "    units_table_file = os.path.join(units_table_path, units_files[0])\n",
    "    units_table = pd.read_pickle(units_table_file)\n",
    "    print(f\"Loaded units table from {units_table_file}\")\n",
    "\n",
    "    # Modify units_table to match final analysis_table format \n",
    "    # Remove unnecessary columns \n",
    "    exclude_columns = ['sessionID', 'mouseID', 'genotype', 'experiment', 'recording', 'recording_date', 'probe_config', 'spike_times', '10ms_pulse_psth', '200ms_pulse_psth']    \n",
    "    analysis_table = units_table.drop(columns=exclude_columns, errors='ignore')\n",
    "\n",
    "    # Rename columns \n",
    "    analysis_table = analysis_table.rename(columns={\n",
    "        'amp': 'amplitude', \n",
    "        'dur': 'duration', \n",
    "        'PTR': 'peak_to_trough_ratio',\n",
    "        'prePTR': 'pre_peak_to_trough_ratio',\n",
    "        'repol_slope': 'repolarization_slope',\n",
    "        'recov_slope': 'recovery_slope',\n",
    "        'ct': 'cell_type'\n",
    "    })\n",
    "\n",
    "    return analysis_table\n",
    "\n",
    "def extract_stimulus_table(session_folder: str) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Extract stimulus table from session folder and add stimulus information from metadata.\n",
    "    \n",
    "    Args:\n",
    "        session_folder: Path to session directory\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with stimulus data or None if not found\n",
    "    \"\"\"\n",
    "    stim_path = os.path.join(session_folder, \"processed_data\", \"stim\")\n",
    "    \n",
    "    if not os.path.exists(stim_path):\n",
    "        print(f\"Stimulus folder does not exist in {session_folder}\")\n",
    "        return None\n",
    "    \n",
    "    # Initialize stimulus table\n",
    "    stim_table = pd.DataFrame(columns=['start_time', 'stop_time', 'power', 'stim_name', \n",
    "                                     'probe', 'wavelength'])\n",
    "    \n",
    "    # Load stimulus table from opto_stim_df.csv\n",
    "    stim_table_path = os.path.join(stim_path, \"opto_stim_df.csv\")\n",
    "    if not os.path.exists(stim_table_path):\n",
    "        print(f\"No stimulus table files found in {stim_path}\")\n",
    "        return None\n",
    "    \n",
    "    opto_stim_df = pd.read_csv(stim_table_path)\n",
    "    print(f\"Loaded stimulus table from {stim_table_path}\")\n",
    "    \n",
    "    # Populate stimulus table\n",
    "    stim_table['start_time'] = opto_stim_df['stim_on']\n",
    "    stim_table['stop_time'] = opto_stim_df['stim_off']\n",
    "    stim_table['stim_name'] = opto_stim_df['epoch_label']\n",
    "    stim_table['probe'] = opto_stim_df['probe']\n",
    "    \n",
    "    # Extract more stimulus information from session.json metadata \n",
    "    metadata_path = os.path.join(session_folder, \"metadata\", \"session.json\")\n",
    "    if os.path.exists(metadata_path):\n",
    "        with open(metadata_path, 'r') as f:\n",
    "            metadata = json.load(f)\n",
    "            \n",
    "        # Extract wavelength and power from first stimulus epoch light_source_config \n",
    "        wavelength = None\n",
    "        power = None\n",
    "        for epoch in metadata.get('stimulus_epochs', []):\n",
    "            light_config = epoch.get('light_source_config')\n",
    "            if light_config is not None:\n",
    "                wavelength = light_config.get('wavelength')\n",
    "                power = light_config.get('laser_power')\n",
    "                break # Only store first epoch's config as the rest are redundant \n",
    "        \n",
    "        stim_table['wavelength'] = wavelength\n",
    "        stim_table['power'] = power\n",
    "    \n",
    "    return stim_table\n",
    "\n",
    "def create_epoch_table(session_folder: str) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Create epoch table from stimulus epoch files and metadata.\n",
    "    \n",
    "    Args:\n",
    "        session_folder: Path to session directory\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with epoch information\n",
    "    \"\"\"\n",
    "    epoch_path = os.path.join(session_folder, \"processed_data\", \"stim\")\n",
    "    \n",
    "    if not os.path.exists(epoch_path):\n",
    "        print(f\"Stimulus folder does not exist in {session_folder}\")\n",
    "        return None\n",
    "    \n",
    "    # Process each CSV file that contains trial by trial timing information for each stimulus epoch\n",
    "    all_epoch_data = []\n",
    "    epoch_files = [f for f in os.listdir(epoch_path) if f.endswith('.csv')]\n",
    "    \n",
    "    if not epoch_files:\n",
    "        print(f\"No CSV files found in {epoch_path}\")\n",
    "        return None\n",
    "    \n",
    "    # Each CSV corresponds to a different stimulus epoch type \n",
    "    for epoch_file in epoch_files:\n",
    "        file_path = os.path.join(epoch_path, epoch_file)\n",
    "        \n",
    "        # Read required columns, handle missing stim_off \n",
    "        # Only the OptoTagging epochs have a stim_off column, others do not \n",
    "        try:\n",
    "            temp_df = pd.read_csv(file_path, usecols=['stim_on', 'stim_off', 'epoch_label'])\n",
    "        except ValueError:\n",
    "            temp_df = pd.read_csv(file_path, usecols=['stim_on', 'epoch_label'])\n",
    "            temp_df['stim_off'] = pd.NA\n",
    "        \n",
    "        all_epoch_data.append(temp_df)\n",
    "    \n",
    "    # Combine trial-by-trial information across all stimulus epochs\n",
    "    combined_df = pd.concat(all_epoch_data, ignore_index=True)\n",
    "    \n",
    "    # Group combined dataframe by epoch_label to get start/stop times for the entire epoch \n",
    "    epoch_summary = combined_df.groupby('epoch_label').apply(\n",
    "        lambda group: pd.Series({\n",
    "            'start_time': group['stim_on'].min(),\n",
    "            'stop_time': group['stim_off'].max() if 'OptoTagging' in group.name else group['stim_on'].max()\n",
    "        }) \n",
    "    ).reset_index() # Use max for stim_off only if it exists, otherwise use max of stim_on\n",
    "    \n",
    "    # Rename columns \n",
    "    epoch_table = epoch_summary.rename(columns={'epoch_label': 'stim_name'})\n",
    "    \n",
    "    # Sort epochs by start_time \n",
    "    epoch_table = epoch_table.sort_values(by='start_time').reset_index(drop=True)\n",
    "    \n",
    "    return epoch_table\n",
    "\n",
    "def add_injection_epoch(epoch_table: pd.DataFrame, session_folder: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Add injection epoch between OptoTagging_0 and Spontaneous_1. \n",
    "    \n",
    "    Args:\n",
    "        epoch_table: Existing epoch table\n",
    "        session_folder: Path to session directory\n",
    "    \n",
    "    Returns:\n",
    "        Modified epoch table with injection epoch added\n",
    "    \"\"\"\n",
    "    metadata_path = os.path.join(session_folder, \"metadata\", \"session.json\")\n",
    "    \n",
    "    if not os.path.exists(metadata_path):\n",
    "        print(f\"Metadata file does not exist in {metadata_path}\")\n",
    "        return epoch_table\n",
    "    \n",
    "    with open(metadata_path, 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "    \n",
    "    # Determine injection type from OptoTagging stimulus epoch note \n",
    "    injection_name = \"None\"  # Default if missing info in metadata \n",
    "    for epoch in metadata.get('stimulus_epochs', []): # If saline or psilocybin injection noted, add Injection_Time epoch \n",
    "        if epoch.get('stimulus_name') == 'OptoTagging':\n",
    "            notes = epoch.get('notes', '')\n",
    "            if 'Saline' in notes:\n",
    "                injection_name = \"Injection_Time\"\n",
    "            elif 'Psilocybin' in notes:\n",
    "                injection_name = \"Injection_Time\"\n",
    "            break\n",
    "    \n",
    "    # Get start and stop times for new epoch based on the timing of Optotagging_0 and Spontaneous_1 epochs \n",
    "    opto_0_stop = None\n",
    "    spont_1_start = None\n",
    "    \n",
    "    for idx, row in epoch_table.iterrows():\n",
    "        if row['stim_name'] == 'OptoTagging_0': # Get value of stop_time for OptoTagging_0\n",
    "            opto_0_stop = row['stop_time']\n",
    "        elif row['stim_name'] == 'Spontaneous_1': # Get value of start_time for Spontaneous_1 \n",
    "            spont_1_start = row['start_time']\n",
    "    \n",
    "    if opto_0_stop is not None and spont_1_start is not None:\n",
    "        # Create injection epoch with time buffer \n",
    "        injection_start = opto_0_stop + 0.01\n",
    "        injection_end = spont_1_start - 0.01\n",
    "        \n",
    "        # Identify the correct index in epoch_table to insert the new epoch (after Optotagging_0)\n",
    "        insert_idx = None\n",
    "        for idx, row in epoch_table.iterrows():\n",
    "            if row['stim_name'] == 'OptoTagging_0':\n",
    "                insert_idx = idx + 1\n",
    "                break\n",
    "        \n",
    "        # Fill out the new row with injection information \n",
    "        new_row = pd.DataFrame({\n",
    "            'stim_name': [injection_name],\n",
    "            'start_time': [injection_start],\n",
    "            'stop_time': [injection_end]\n",
    "        })\n",
    "        \n",
    "        # Insert the new row at the correct index into the epoch_table \n",
    "        if insert_idx is not None:\n",
    "            epoch_table = pd.concat([\n",
    "                epoch_table.iloc[:insert_idx], # Keep rows before the insertion point \n",
    "                new_row, \n",
    "                epoch_table.iloc[insert_idx:] # Keep rows after the insertion point \n",
    "            ], ignore_index=True)\n",
    "            print(f\"Added {injection_name} epoch from {injection_start} to {injection_end}\")\n",
    "    \n",
    "    return epoch_table\n",
    "\n",
    "def convert_dataframe_to_dynamic_table(df: pd.DataFrame, table_name: str) -> DynamicTable:\n",
    "    \"\"\"\n",
    "    Convert pandas DataFrame to DynamicTable object compatible with NWB files. \n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        table_name: Name for the dynamic table\n",
    "    \n",
    "    Returns:\n",
    "        DynamicTable object\n",
    "    \"\"\"\n",
    "    dynamic_table = DynamicTable.from_dataframe(\n",
    "        name=table_name.lower().replace(' ', '_'),\n",
    "        df=df\n",
    "    )\n",
    "    # name ensures the input is lowercase and has underscores  \n",
    "    print(f\"Created DynamicTable '{table_name}' with {len(df)} rows and {len(df.columns)} columns\")\n",
    "    return dynamic_table\n",
    "\n",
    "def modify_nwb_file(original_path: str, new_path: str, analysis_table: Optional[pd.DataFrame] = None,\n",
    "                   stim_table: Optional[pd.DataFrame] = None, epoch_table: Optional[pd.DataFrame] = None) -> None:\n",
    "    \"\"\"\n",
    "    Modify existing NWB file with new tables and save to new location. \n",
    "    Tables are pandas DataFrames that will be converted to DynamicTables. \n",
    "    \n",
    "    Args:\n",
    "        original_path: Path to input NWB file\n",
    "        new_path: Path for output NWB file\n",
    "        analysis_table: Analysis table to add\n",
    "        stim_table: Stimulus table to add\n",
    "        epoch_table: Epoch table to add\n",
    "    \"\"\"\n",
    "    # Create directory for new file\n",
    "    os.makedirs(os.path.dirname(new_path), exist_ok=True)\n",
    "    \n",
    "    # Open existing NWB file\n",
    "    io = hdmf_zarr.NWBZarrIO(original_path, mode=\"r\")\n",
    "    nwbfile = io.read()\n",
    "    \n",
    "    # Add analysis table\n",
    "    if analysis_table is not None:\n",
    "        print(\"Processing analysis table...\")\n",
    "        try:\n",
    "            analysis_dynamic_table = convert_dataframe_to_dynamic_table(\n",
    "                df=analysis_table,\n",
    "                table_name=\"analysis_table\"\n",
    "            )\n",
    "            nwbfile.add_analysis(analysis_dynamic_table)\n",
    "            print(f\"Added analysis table with {len(analysis_table)} rows\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to add analysis table: {e}\")\n",
    "            raise\n",
    "    \n",
    "    # Add stimulus table\n",
    "    if stim_table is not None:\n",
    "        print(\"Processing stimulus table...\")\n",
    "        try:\n",
    "            stimulus_dynamic_table = convert_dataframe_to_dynamic_table(\n",
    "                df=stim_table,\n",
    "                table_name=\"stimulus_table\"\n",
    "            )\n",
    "            nwbfile.add_stimulus(stimulus_dynamic_table)\n",
    "            print(f\"Added stimulus table with {len(stim_table)} rows\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to add stimulus table: {e}\")\n",
    "            raise\n",
    "    \n",
    "    # Add epoch table\n",
    "    if epoch_table is not None:\n",
    "        print(\"Processing epoch table...\")\n",
    "        try: # Constructs epoch table row by row from DataFrame  \n",
    "            for idx, row in epoch_table.iterrows():\n",
    "                tags = str(row['stim_name']) if 'stim_name' in row and pd.notna(row['stim_name']) else []\n",
    "                nwbfile.add_epoch(\n",
    "                    start_time=float(row['start_time']),\n",
    "                    stop_time=float(row['stop_time']),                    \n",
    "                    tags = tags \n",
    "                )\n",
    "            print(f\"Added {len(epoch_table)} epochs\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to add epoch table: {e}\")\n",
    "            raise\n",
    "    \n",
    "    # Write modified NWB to new file \n",
    "    with hdmf_zarr.NWBZarrIO(new_path, mode='w') as export_io:\n",
    "        export_io.export(src_io=io, nwbfile=nwbfile)\n",
    "    \n",
    "    io.close()\n",
    "    print(f\"Saved to: {new_path}\")\n",
    "\n",
    "def process_single_session(session_name: str, base_path: str, output_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Process a single session through the complete pipeline. Finds correct file paths based on session_name for all necessary directories and files. \n",
    "    \n",
    "    Args:\n",
    "        session_name: Name of the session to process\n",
    "        base_path: Base path containing session folders\n",
    "        output_path: Output directory for modified NWB files\n",
    "    \"\"\"\n",
    "    session_folder = os.path.join(base_path, session_name)\n",
    "    print(f\"\\nProcessing session: {session_name}\")\n",
    "    \n",
    "    if not os.path.exists(session_folder):\n",
    "        print(f\"Session folder {session_folder} does not exist\")\n",
    "        return\n",
    "    \n",
    "    # Extract all tables\n",
    "    analysis_table = extract_analysis_table(session_folder)\n",
    "    stim_table = extract_stimulus_table(session_folder)\n",
    "    epoch_table = create_epoch_table(session_folder)\n",
    "    \n",
    "    # Add injection epoch if epoch table exists\n",
    "    if epoch_table is not None:\n",
    "        epoch_table = add_injection_epoch(epoch_table, session_folder)\n",
    "    \n",
    "    # Find original NWB file\n",
    "    nwb_search_path = os.path.join(session_folder, f\"*{session_name.split('_')[1]}*\")\n",
    "    \n",
    "    # First, find directories containing the session name\n",
    "    cleaned_session_name = session_name[0].replace('-', '')\n",
    "    # Within the session folder, search for directories that contain the cleaned session name\n",
    "    session_dirs = glob.glob(os.path.join(session_folder, f\"*{cleaned_session_name}*\")) \n",
    "    if not session_dirs:\n",
    "        print(f\"No directory containing '{cleaned_session_name}' found in {session_folder}\")\n",
    "\n",
    "    # Then search for NWB files containing the session name within those directories\n",
    "    nwb_files = []\n",
    "    for session_dir in session_dirs:\n",
    "        # instead, search for a pattern that ends in .nwb \n",
    "        nwb_pattern = os.path.join(session_dir, f\"*.nwb\")\n",
    "        # Get full file path to nwb file \n",
    "        nwb_files.extend(glob.glob(nwb_pattern))\n",
    "        # Get nwb file name to match for output file \n",
    "        nwb_files.extend([f for f in os.listdir(session_dir) if f.endswith('.nwb')])\n",
    "\n",
    "    if not nwb_files:\n",
    "        print(f\"No NWB file containing '{session_name}' found in session directories\")\n",
    "\n",
    "    original_nwb = nwb_files[0]  # Take the first match\n",
    "    print(f\"Found NWB file: {original_nwb}\")\n",
    "    new_nwb = os.path.join(output_path, nwb_files[1])\n",
    "    \n",
    "    # Modify NWB file\n",
    "    modify_nwb_file(\n",
    "        original_path=original_nwb,\n",
    "        new_path=new_nwb,\n",
    "        analysis_table=analysis_table,\n",
    "        stim_table=stim_table,\n",
    "        epoch_table=epoch_table\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12 sessions matching criteria\n",
      "\n",
      "Processing session: 2024-05-14_714527\n",
      "Loaded units table from /Volumes/scratch/andrew.shelton/NPUltra_data/raw_npultra_data/2024-05-14_714527/processed_data/units/2024-05-14_714527_1_units_epoch.pkl\n",
      "Loaded stimulus table from /Volumes/scratch/andrew.shelton/NPUltra_data/raw_npultra_data/2024-05-14_714527/processed_data/stim/opto_stim_df.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1f/h0zlr4c90tg4sf8rv_22d7v80000gp/T/ipykernel_32393/1800296004.py:178: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  combined_df = pd.concat(all_epoch_data, ignore_index=True)\n",
      "/var/folders/1f/h0zlr4c90tg4sf8rv_22d7v80000gp/T/ipykernel_32393/1800296004.py:181: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  epoch_summary = combined_df.groupby('epoch_label').apply(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added Injection_Time epoch from 2714.53929 to 3058.6834599999997\n",
      "Found NWB file: /Volumes/scratch/andrew.shelton/NPUltra_data/raw_npultra_data/2024-05-14_714527/20240514_714527_1/ecephys_714527_2024-05-14_13-22-07_experiment1_recording1.nwb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/suyee.lee/np-ultra-psychedelics/.venv/lib/python3.12/site-packages/hdmf/spec/namespace.py:535: UserWarning: Ignoring cached namespace 'core' version 2.6.0-alpha because version 2.8.0 is already loaded.\n",
      "  warn(\"Ignoring cached namespace '%s' version %s because version %s is already loaded.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing analysis table...\n",
      "Created DynamicTable 'analysis_table' with 350 rows and 44 columns\n",
      "Added analysis table with 350 rows\n",
      "Processing stimulus table...\n",
      "Created DynamicTable 'stimulus_table' with 600 rows and 6 columns\n",
      "Added stimulus table with 600 rows\n",
      "Processing epoch table...\n",
      "Added 13 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/suyee.lee/np-ultra-psychedelics/.venv/lib/python3.12/site-packages/zarr/storage.py:455: FutureWarning: missing object_codec for object array; this will raise a ValueError in version 3.0\n",
      "  _init_array_metadata(\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: '/Volumes/scratch/suyee.lee/ecephys_714527_2024-05-14_13-22-07_experiment1_recording1.nwb/units/waveform_mean/0.5.2.a4ba0825b87d4bcebb584d5f2dcb8da1.partial'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPermissionError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Example usage: \u001b[39;00m\n\u001b[32m      2\u001b[39m filtered_session_list = get_sessions_of_interest(\n\u001b[32m      3\u001b[39m     summary_path = \u001b[33m\"\u001b[39m\u001b[33m/Volumes/scratch/andrew.shelton/NPUltra_data/raw_npultra_data/NPUltra_recording_summary.xlsx\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      4\u001b[39m     experiment_filter = \u001b[33m\"\u001b[39m\u001b[33mNPUltra_psychedelics\u001b[39m\u001b[33m\"\u001b[39m, \n\u001b[32m      5\u001b[39m     upload_filter = \u001b[33m'\u001b[39m\u001b[33myes\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      6\u001b[39m     )\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[43mprocess_single_session\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43msession_name\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mfiltered_session_list\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Process one session as an example \u001b[39;49;00m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbase_path\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m/Volumes/scratch/andrew.shelton/NPUltra_data/raw_npultra_data/\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m/Volumes/scratch/suyee.lee\u001b[39;49m\u001b[33;43m'\u001b[39;49m\n\u001b[32m     12\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 410\u001b[39m, in \u001b[36mprocess_single_session\u001b[39m\u001b[34m(session_name, base_path, output_path)\u001b[39m\n\u001b[32m    407\u001b[39m new_nwb = os.path.join(output_path, nwb_files[\u001b[32m1\u001b[39m])\n\u001b[32m    409\u001b[39m \u001b[38;5;66;03m# Modify NWB file\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m410\u001b[39m \u001b[43mmodify_nwb_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    411\u001b[39m \u001b[43m    \u001b[49m\u001b[43moriginal_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43moriginal_nwb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    412\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnew_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_nwb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    413\u001b[39m \u001b[43m    \u001b[49m\u001b[43manalysis_table\u001b[49m\u001b[43m=\u001b[49m\u001b[43manalysis_table\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    414\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstim_table\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstim_table\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    415\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepoch_table\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepoch_table\u001b[49m\n\u001b[32m    416\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 352\u001b[39m, in \u001b[36mmodify_nwb_file\u001b[39m\u001b[34m(original_path, new_path, analysis_table, stim_table, epoch_table)\u001b[39m\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# Write modified NWB to new file \u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m hdmf_zarr.NWBZarrIO(new_path, mode=\u001b[33m'\u001b[39m\u001b[33mw\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m export_io:\n\u001b[32m--> \u001b[39m\u001b[32m352\u001b[39m     \u001b[43mexport_io\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexport\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc_io\u001b[49m\u001b[43m=\u001b[49m\u001b[43mio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnwbfile\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnwbfile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    354\u001b[39m io.close()\n\u001b[32m    355\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSaved to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnew_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/np-ultra-psychedelics/.venv/lib/python3.12/site-packages/hdmf/utils.py:668\u001b[39m, in \u001b[36mdocval.<locals>.dec.<locals>.func_call\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    666\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfunc_call\u001b[39m(*args, **kwargs):\n\u001b[32m    667\u001b[39m     pargs = _check_args(args, kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m668\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/np-ultra-psychedelics/.venv/lib/python3.12/site-packages/hdmf_zarr/nwb.py:71\u001b[39m, in \u001b[36mNWBZarrIO.export\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m     69\u001b[39m nwbfile = popargs(\u001b[33m\"\u001b[39m\u001b[33mnwbfile\u001b[39m\u001b[33m\"\u001b[39m, kwargs)\n\u001b[32m     70\u001b[39m kwargs[\u001b[33m\"\u001b[39m\u001b[33mcontainer\u001b[39m\u001b[33m\"\u001b[39m] = nwbfile\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexport\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/np-ultra-psychedelics/.venv/lib/python3.12/site-packages/hdmf/utils.py:668\u001b[39m, in \u001b[36mdocval.<locals>.dec.<locals>.func_call\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    666\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfunc_call\u001b[39m(*args, **kwargs):\n\u001b[32m    667\u001b[39m     pargs = _check_args(args, kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m668\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/np-ultra-psychedelics/.venv/lib/python3.12/site-packages/hdmf_zarr/backend.py:433\u001b[39m, in \u001b[36mZarrIO.export\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    431\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m write_args.get(\u001b[33m\"\u001b[39m\u001b[33mlink_data\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m    432\u001b[39m     ckwargs[\u001b[33m\"\u001b[39m\u001b[33mclear_cache\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m433\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexport\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mckwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    434\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cache_spec:\n\u001b[32m    435\u001b[39m     \u001b[38;5;66;03m# add any namespaces from the src_io that have not yet been loaded\u001b[39;00m\n\u001b[32m    436\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m namespace \u001b[38;5;129;01min\u001b[39;00m src_io.manager.namespace_catalog.namespaces:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/np-ultra-psychedelics/.venv/lib/python3.12/site-packages/hdmf/utils.py:668\u001b[39m, in \u001b[36mdocval.<locals>.dec.<locals>.func_call\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    666\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfunc_call\u001b[39m(*args, **kwargs):\n\u001b[32m    667\u001b[39m     pargs = _check_args(args, kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m668\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/np-ultra-psychedelics/.venv/lib/python3.12/site-packages/hdmf/backends/io.py:166\u001b[39m, in \u001b[36mHDMFIO.export\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    164\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    165\u001b[39m     bldr = src_io.read_builder()\n\u001b[32m--> \u001b[39m\u001b[32m166\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwrite_builder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuilder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbldr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mwrite_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/np-ultra-psychedelics/.venv/lib/python3.12/site-packages/hdmf/utils.py:668\u001b[39m, in \u001b[36mdocval.<locals>.dec.<locals>.func_call\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    666\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfunc_call\u001b[39m(*args, **kwargs):\n\u001b[32m    667\u001b[39m     pargs = _check_args(args, kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m668\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/np-ultra-psychedelics/.venv/lib/python3.12/site-packages/hdmf_zarr/backend.py:517\u001b[39m, in \u001b[36mZarrIO.write_builder\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    513\u001b[39m f_builder, link_data, exhaust_dci, export_source, consolidate_metadata = getargs(\n\u001b[32m    514\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mbuilder\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mlink_data\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mexhaust_dci\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mexport_source\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mconsolidate_metadata\u001b[39m\u001b[33m\"\u001b[39m, kwargs\n\u001b[32m    515\u001b[39m )\n\u001b[32m    516\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, gbldr \u001b[38;5;129;01min\u001b[39;00m f_builder.groups.items():\n\u001b[32m--> \u001b[39m\u001b[32m517\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwrite_group\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    518\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparent\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    519\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbuilder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgbldr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    520\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlink_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlink_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    521\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexhaust_dci\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexhaust_dci\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    522\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexport_source\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexport_source\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    523\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    524\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, dbldr \u001b[38;5;129;01min\u001b[39;00m f_builder.datasets.items():\n\u001b[32m    525\u001b[39m     \u001b[38;5;28mself\u001b[39m.write_dataset(\n\u001b[32m    526\u001b[39m         parent=\u001b[38;5;28mself\u001b[39m.__file,\n\u001b[32m    527\u001b[39m         builder=dbldr,\n\u001b[32m   (...)\u001b[39m\u001b[32m    530\u001b[39m         export_source=export_source,\n\u001b[32m    531\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/np-ultra-psychedelics/.venv/lib/python3.12/site-packages/hdmf/utils.py:668\u001b[39m, in \u001b[36mdocval.<locals>.dec.<locals>.func_call\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    666\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfunc_call\u001b[39m(*args, **kwargs):\n\u001b[32m    667\u001b[39m     pargs = _check_args(args, kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m668\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/np-ultra-psychedelics/.venv/lib/python3.12/site-packages/hdmf_zarr/backend.py:627\u001b[39m, in \u001b[36mZarrIO.write_group\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    625\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m datasets:\n\u001b[32m    626\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m dset_name, sub_builder \u001b[38;5;129;01min\u001b[39;00m datasets.items():\n\u001b[32m--> \u001b[39m\u001b[32m627\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwrite_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m            \u001b[49m\u001b[43mparent\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    629\u001b[39m \u001b[43m            \u001b[49m\u001b[43mbuilder\u001b[49m\u001b[43m=\u001b[49m\u001b[43msub_builder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    630\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlink_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlink_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    631\u001b[39m \u001b[43m            \u001b[49m\u001b[43mexhaust_dci\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexhaust_dci\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    632\u001b[39m \u001b[43m            \u001b[49m\u001b[43mexport_source\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexport_source\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    633\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    635\u001b[39m links = builder.links\n\u001b[32m    636\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m links:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/np-ultra-psychedelics/.venv/lib/python3.12/site-packages/hdmf/utils.py:668\u001b[39m, in \u001b[36mdocval.<locals>.dec.<locals>.func_call\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    666\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfunc_call\u001b[39m(*args, **kwargs):\n\u001b[32m    667\u001b[39m     pargs = _check_args(args, kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m668\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/np-ultra-psychedelics/.venv/lib/python3.12/site-packages/hdmf_zarr/backend.py:1075\u001b[39m, in \u001b[36mZarrIO.write_dataset\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m   1069\u001b[39m             dset = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1071\u001b[39m         \u001b[38;5;66;03m###############\u001b[39;00m\n\u001b[32m   1072\u001b[39m         \u001b[38;5;66;03m# Case 3: The dataset is in the export source and has the SAME path as the builder, so copy.\u001b[39;00m\n\u001b[32m   1073\u001b[39m         \u001b[38;5;66;03m###############\u001b[39;00m\n\u001b[32m   1074\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1075\u001b[39m             \u001b[43mzarr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1076\u001b[39m             dset = parent[name]\n\u001b[32m   1077\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/np-ultra-psychedelics/.venv/lib/python3.12/site-packages/zarr/convenience.py:906\u001b[39m, in \u001b[36mcopy\u001b[39m\u001b[34m(source, dest, name, shallow, without_attrs, log, if_exists, dry_run, **create_kws)\u001b[39m\n\u001b[32m    903\u001b[39m \u001b[38;5;66;03m# setup logging\u001b[39;00m\n\u001b[32m    904\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m _LogWriter(log) \u001b[38;5;28;01mas\u001b[39;00m log:\n\u001b[32m    905\u001b[39m     \u001b[38;5;66;03m# do the copying\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m906\u001b[39m     n_copied, n_skipped, n_bytes_copied = \u001b[43m_copy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    907\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlog\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    908\u001b[39m \u001b[43m        \u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    909\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    910\u001b[39m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    911\u001b[39m \u001b[43m        \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    912\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshallow\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshallow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    913\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwithout_attrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwithout_attrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    914\u001b[39m \u001b[43m        \u001b[49m\u001b[43mif_exists\u001b[49m\u001b[43m=\u001b[49m\u001b[43mif_exists\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdry_run\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdry_run\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcreate_kws\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    919\u001b[39m     \u001b[38;5;66;03m# log a final message with a summary of what happened\u001b[39;00m\n\u001b[32m    920\u001b[39m     _log_copy_summary(log, dry_run, n_copied, n_skipped, n_bytes_copied)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/np-ultra-psychedelics/.venv/lib/python3.12/site-packages/zarr/convenience.py:1021\u001b[39m, in \u001b[36m_copy\u001b[39m\u001b[34m(log, source, dest, name, root, shallow, without_attrs, if_exists, dry_run, **create_kws)\u001b[39m\n\u001b[32m   1019\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m offset \u001b[38;5;129;01min\u001b[39;00m itertools.product(*chunk_offsets):\n\u001b[32m   1020\u001b[39m     sel = \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mslice\u001b[39m(o, \u001b[38;5;28mmin\u001b[39m(s, o + c)) \u001b[38;5;28;01mfor\u001b[39;00m o, s, c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(offset, shape, chunks))\n\u001b[32m-> \u001b[39m\u001b[32m1021\u001b[39m     \u001b[43mds\u001b[49m\u001b[43m[\u001b[49m\u001b[43msel\u001b[49m\u001b[43m]\u001b[49m = source[sel]\n\u001b[32m   1022\u001b[39m n_bytes_copied += ds.size * ds.dtype.itemsize\n\u001b[32m   1024\u001b[39m \u001b[38;5;66;03m# copy attributes\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/np-ultra-psychedelics/.venv/lib/python3.12/site-packages/zarr/core.py:1449\u001b[39m, in \u001b[36mArray.__setitem__\u001b[39m\u001b[34m(self, selection, value)\u001b[39m\n\u001b[32m   1447\u001b[39m     \u001b[38;5;28mself\u001b[39m.vindex[selection] = value\n\u001b[32m   1448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m is_pure_orthogonal_indexing(pure_selection, \u001b[38;5;28mself\u001b[39m.ndim):\n\u001b[32m-> \u001b[39m\u001b[32m1449\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mset_orthogonal_selection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpure_selection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfields\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfields\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1450\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1451\u001b[39m     \u001b[38;5;28mself\u001b[39m.set_basic_selection(pure_selection, value, fields=fields)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/np-ultra-psychedelics/.venv/lib/python3.12/site-packages/zarr/core.py:1638\u001b[39m, in \u001b[36mArray.set_orthogonal_selection\u001b[39m\u001b[34m(self, selection, value, fields)\u001b[39m\n\u001b[32m   1635\u001b[39m \u001b[38;5;66;03m# setup indexer\u001b[39;00m\n\u001b[32m   1636\u001b[39m indexer = OrthogonalIndexer(selection, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1638\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_set_selection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfields\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfields\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/np-ultra-psychedelics/.venv/lib/python3.12/site-packages/zarr/core.py:1990\u001b[39m, in \u001b[36mArray._set_selection\u001b[39m\u001b[34m(self, indexer, value, fields)\u001b[39m\n\u001b[32m   1987\u001b[39m                 chunk_value = chunk_value[item]\n\u001b[32m   1989\u001b[39m         \u001b[38;5;66;03m# put data\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1990\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_chunk_setitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk_coords\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_selection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfields\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfields\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1991\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1992\u001b[39m     lchunk_coords, lchunk_selection, lout_selection = \u001b[38;5;28mzip\u001b[39m(*indexer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/np-ultra-psychedelics/.venv/lib/python3.12/site-packages/zarr/core.py:2263\u001b[39m, in \u001b[36mArray._chunk_setitem\u001b[39m\u001b[34m(self, chunk_coords, chunk_selection, value, fields)\u001b[39m\n\u001b[32m   2260\u001b[39m     lock = \u001b[38;5;28mself\u001b[39m._synchronizer[ckey]\n\u001b[32m   2262\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m lock:\n\u001b[32m-> \u001b[39m\u001b[32m2263\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_chunk_setitem_nosync\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk_coords\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_selection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfields\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfields\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/np-ultra-psychedelics/.venv/lib/python3.12/site-packages/zarr/core.py:2273\u001b[39m, in \u001b[36mArray._chunk_setitem_nosync\u001b[39m\u001b[34m(self, chunk_coords, chunk_selection, value, fields)\u001b[39m\n\u001b[32m   2271\u001b[39m     \u001b[38;5;28mself\u001b[39m._chunk_delitem(ckey)\n\u001b[32m   2272\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2273\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mchunk_store\u001b[49m\u001b[43m[\u001b[49m\u001b[43mckey\u001b[49m\u001b[43m]\u001b[49m = \u001b[38;5;28mself\u001b[39m._encode_chunk(cdata)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/np-ultra-psychedelics/.venv/lib/python3.12/site-packages/zarr/storage.py:1151\u001b[39m, in \u001b[36mDirectoryStore.__setitem__\u001b[39m\u001b[34m(self, key, value)\u001b[39m\n\u001b[32m   1149\u001b[39m temp_path = os.path.join(dir_path, temp_name)\n\u001b[32m   1150\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1151\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_tofile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemp_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1153\u001b[39m     \u001b[38;5;66;03m# move temporary file into place;\u001b[39;00m\n\u001b[32m   1154\u001b[39m     \u001b[38;5;66;03m# make several attempts at writing the temporary file to get past\u001b[39;00m\n\u001b[32m   1155\u001b[39m     \u001b[38;5;66;03m# potential antivirus file locking issues\u001b[39;00m\n\u001b[32m   1156\u001b[39m     retry_call(os.replace, (temp_path, file_path), exceptions=(\u001b[38;5;167;01mPermissionError\u001b[39;00m,))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/np-ultra-psychedelics/.venv/lib/python3.12/site-packages/zarr/storage.py:1111\u001b[39m, in \u001b[36mDirectoryStore._tofile\u001b[39m\u001b[34m(a, fn)\u001b[39m\n\u001b[32m   1095\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m   1096\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_tofile\u001b[39m(a, fn):\n\u001b[32m   1097\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Write data to a file\u001b[39;00m\n\u001b[32m   1098\u001b[39m \n\u001b[32m   1099\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1109\u001b[39m \u001b[33;03m    file writing logic.\u001b[39;00m\n\u001b[32m   1110\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1111\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mwb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m   1112\u001b[39m         f.write(a)\n",
      "\u001b[31mPermissionError\u001b[39m: [Errno 13] Permission denied: '/Volumes/scratch/suyee.lee/ecephys_714527_2024-05-14_13-22-07_experiment1_recording1.nwb/units/waveform_mean/0.5.2.a4ba0825b87d4bcebb584d5f2dcb8da1.partial'"
     ]
    }
   ],
   "source": [
    "# Example usage: \n",
    "filtered_session_list = get_sessions_of_interest(\n",
    "    summary_path = \"/Volumes/scratch/andrew.shelton/NPUltra_data/raw_npultra_data/NPUltra_recording_summary.xlsx\",\n",
    "    experiment_filter = \"NPUltra_psychedelics\", \n",
    "    upload_filter = 'yes'\n",
    "    )\n",
    "\n",
    "process_single_session(\n",
    "    session_name = filtered_session_list[0],  # Process one session as an example \n",
    "    base_path = '/Volumes/scratch/andrew.shelton/NPUltra_data/raw_npultra_data/',\n",
    "    output_path = '/Volumes/scratch/suyee.lee'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded new NWB file: /Volumes/aind/scratch/suyee.lee/ecephys_714527_2024-05-14_13-22-07_experiment1_recording1.nwb\n",
      "Processing modules: []\n",
      "Stimulus presentations and epochs: ['stimulus_table']\n",
      "Epochs: epochs pynwb.epoch.TimeIntervals at 0x4685171792\n",
      "Fields:\n",
      "  colnames: ['start_time' 'stop_time' 'tags']\n",
      "  columns: (\n",
      "    start_time <class 'hdmf.common.table.VectorData'>,\n",
      "    stop_time <class 'hdmf.common.table.VectorData'>,\n",
      "    tags_index <class 'hdmf.common.table.VectorIndex'>,\n",
      "    tags <class 'hdmf.common.table.VectorData'>\n",
      "  )\n",
      "  description: experimental epochs\n",
      "  id: id <class 'hdmf.common.table.ElementIdentifiers'>\n",
      "\n",
      "Trials: None\n"
     ]
    }
   ],
   "source": [
    "# Check outputs of new nwb file \n",
    "# \n",
    "new_nwb = '/Volumes/aind/scratch/suyee.lee/ecephys_714527_2024-05-14_13-22-07_experiment1_recording1.nwb'\n",
    "\n",
    "with hdmf_zarr.NWBZarrIO(new_nwb, mode='r') as io:\n",
    "    new_nwbfile = io.read()\n",
    "    print(\"Loaded new NWB file:\", new_nwb)\n",
    "    print(\"Processing modules:\", list(new_nwbfile.processing.keys()))\n",
    "    print(\"Stimulus presentations and epochs:\", list(new_nwbfile.stimulus.keys()) if hasattr(new_nwbfile, 'stimulus') else \"None\")\n",
    "    print(\"Epochs:\", new_nwbfile.epochs)  # Should show the added epochs\n",
    "    print(\"Trials:\", new_nwbfile.trials)  # Should show the added trials\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remaining to do: \n",
    "# Write unit tests to check the outputs of each table and NWBFile \n",
    "# Unpack and modify units table, check the organization x \n",
    "# Figure out how to add the new units table to the NWB file \n",
    "# Fix the epoch table label so that it's a string not [string] (How do I describe that?)\n",
    "# Add another stimulus table for RFMapping. check in the hdf5 if there's a stim off variable to include. \n",
    "\n",
    "# note updated the tag to be a string instead of a list, this introduced an error might bring back the [ ] when grabbing the\n",
    "tags = [str(row['stim_name'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12 sessions matching criteria\n"
     ]
    }
   ],
   "source": [
    "filtered_session_list = get_sessions_of_interest(\n",
    "    summary_path = \"/Volumes/scratch/andrew.shelton/NPUltra_data/raw_npultra_data/NPUltra_recording_summary.xlsx\",\n",
    "    experiment_filter = \"NPUltra_psychedelics\", \n",
    "    upload_filter = 'yes'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2024-05-14_714527'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/suyee.lee/np-ultra-psychedelics/.venv/lib/python3.12/site-packages/hdmf/spec/namespace.py:535: UserWarning: Ignoring cached namespace 'core' version 2.6.0-alpha because version 2.8.0 is already loaded.\n",
      "  warn(\"Ignoring cached namespace '%s' version %s because version %s is already loaded.\"\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[51]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     25\u001b[39m     nwbfile = io.read()\n\u001b[32m     26\u001b[39m     \u001b[38;5;66;03m# Get units table \u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m     units_table = \u001b[43mnwbfile\u001b[49m\u001b[43m.\u001b[49m\u001b[43munits\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Modify units table \u001b[39;00m\n\u001b[32m     30\u001b[39m modified_units_table = units_table.copy()  \u001b[38;5;66;03m# Create a copy to avoid modifying the original\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/np-ultra-psychedelics/.venv/lib/python3.12/site-packages/hdmf/utils.py:668\u001b[39m, in \u001b[36mdocval.<locals>.dec.<locals>.func_call\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    666\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfunc_call\u001b[39m(*args, **kwargs):\n\u001b[32m    667\u001b[39m     pargs = _check_args(args, kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m668\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/np-ultra-psychedelics/.venv/lib/python3.12/site-packages/hdmf/common/table.py:1225\u001b[39m, in \u001b[36mDynamicTable.to_dataframe\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m   1217\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1218\u001b[39m \u001b[33;03mProduce a pandas DataFrame containing this table's data.\u001b[39;00m\n\u001b[32m   1219\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1222\u001b[39m \u001b[33;03mIf exclude is None, this is equivalent to table.get(slice(None, None, None), index=False).\u001b[39;00m\n\u001b[32m   1223\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1224\u001b[39m arg = \u001b[38;5;28mslice\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# select all rows\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1225\u001b[39m sel = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_selection_as_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1226\u001b[39m ret = \u001b[38;5;28mself\u001b[39m.__get_selection_as_df(sel)\n\u001b[32m   1227\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/np-ultra-psychedelics/.venv/lib/python3.12/site-packages/hdmf/common/table.py:1063\u001b[39m, in \u001b[36mDynamicTable.__get_selection_as_dict\u001b[39m\u001b[34m(self, arg, df, index, exclude, **kwargs)\u001b[39m\n\u001b[32m   1061\u001b[39m             ret[name] = col.get(arg, df=\u001b[38;5;28;01mFalse\u001b[39;00m, index=\u001b[38;5;28;01mTrue\u001b[39;00m, **kwargs)\n\u001b[32m   1062\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1063\u001b[39m             ret[name] = \u001b[43mcol\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1064\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\n\u001b[32m   1065\u001b[39m \u001b[38;5;66;03m# if index is out of range, different errors can be generated depending on the dtype of the column\u001b[39;00m\n\u001b[32m   1066\u001b[39m \u001b[38;5;66;03m# but despite the differences, raise an IndexError from that error\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/np-ultra-psychedelics/.venv/lib/python3.12/site-packages/hdmf/common/table.py:203\u001b[39m, in \u001b[36mVectorIndex.get\u001b[39m\u001b[34m(self, arg, **kwargs)\u001b[39m\n\u001b[32m    201\u001b[39m ret = \u001b[38;5;28mlist\u001b[39m()\n\u001b[32m    202\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m indices:\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m     ret.append(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__getitem_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/np-ultra-psychedelics/.venv/lib/python3.12/site-packages/hdmf/common/table.py:173\u001b[39m, in \u001b[36mVectorIndex.__getitem_helper\u001b[39m\u001b[34m(self, arg, **kwargs)\u001b[39m\n\u001b[32m    171\u001b[39m start = \u001b[32m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m arg == \u001b[32m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.data[arg - \u001b[32m1\u001b[39m]\n\u001b[32m    172\u001b[39m end = \u001b[38;5;28mself\u001b[39m.data[arg]\n\u001b[32m--> \u001b[39m\u001b[32m173\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mslice\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/np-ultra-psychedelics/.venv/lib/python3.12/site-packages/hdmf/common/table.py:61\u001b[39m, in \u001b[36mVectorData.get\u001b[39m\u001b[34m(self, key, **kwargs)\u001b[39m\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, **kwargs):\n\u001b[32m     55\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[33;03m    Retrieve elements from this VectorData\u001b[39;00m\n\u001b[32m     57\u001b[39m \n\u001b[32m     58\u001b[39m \u001b[33;03m    :param key: Selection of the elements\u001b[39;00m\n\u001b[32m     59\u001b[39m \u001b[33;03m    :param kwargs: Ignored\u001b[39;00m\n\u001b[32m     60\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/np-ultra-psychedelics/.venv/lib/python3.12/site-packages/hdmf/container.py:1012\u001b[39m, in \u001b[36mData.get\u001b[39m\u001b[34m(self, args)\u001b[39m\n\u001b[32m   1009\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.data, h5py.Dataset) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(args, np.ndarray):\n\u001b[32m   1010\u001b[39m     \u001b[38;5;66;03m# This is needed for h5py 2.9 compatibility\u001b[39;00m\n\u001b[32m   1011\u001b[39m     args = args.tolist()\n\u001b[32m-> \u001b[39m\u001b[32m1012\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43margs\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/np-ultra-psychedelics/.venv/lib/python3.12/site-packages/zarr/core.py:799\u001b[39m, in \u001b[36mArray.__getitem__\u001b[39m\u001b[34m(self, selection)\u001b[39m\n\u001b[32m    797\u001b[39m     result = \u001b[38;5;28mself\u001b[39m.get_orthogonal_selection(pure_selection, fields=fields)\n\u001b[32m    798\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m799\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_basic_selection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpure_selection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfields\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfields\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/np-ultra-psychedelics/.venv/lib/python3.12/site-packages/zarr/core.py:925\u001b[39m, in \u001b[36mArray.get_basic_selection\u001b[39m\u001b[34m(self, selection, out, fields)\u001b[39m\n\u001b[32m    923\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._get_basic_selection_zd(selection=selection, out=out, fields=fields)\n\u001b[32m    924\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m925\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_basic_selection_nd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mselection\u001b[49m\u001b[43m=\u001b[49m\u001b[43mselection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfields\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfields\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/np-ultra-psychedelics/.venv/lib/python3.12/site-packages/zarr/core.py:967\u001b[39m, in \u001b[36mArray._get_basic_selection_nd\u001b[39m\u001b[34m(self, selection, out, fields)\u001b[39m\n\u001b[32m    961\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_basic_selection_nd\u001b[39m(\u001b[38;5;28mself\u001b[39m, selection, out=\u001b[38;5;28;01mNone\u001b[39;00m, fields=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    962\u001b[39m     \u001b[38;5;66;03m# implementation of basic selection for array with at least one dimension\u001b[39;00m\n\u001b[32m    963\u001b[39m \n\u001b[32m    964\u001b[39m     \u001b[38;5;66;03m# setup indexer\u001b[39;00m\n\u001b[32m    965\u001b[39m     indexer = BasicIndexer(selection, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m967\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_selection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfields\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfields\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/np-ultra-psychedelics/.venv/lib/python3.12/site-packages/zarr/core.py:1342\u001b[39m, in \u001b[36mArray._get_selection\u001b[39m\u001b[34m(self, indexer, out, fields)\u001b[39m\n\u001b[32m   1339\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m math.prod(out_shape) > \u001b[32m0\u001b[39m:\n\u001b[32m   1340\u001b[39m     \u001b[38;5;66;03m# allow storage to get multiple items at once\u001b[39;00m\n\u001b[32m   1341\u001b[39m     lchunk_coords, lchunk_selection, lout_selection = \u001b[38;5;28mzip\u001b[39m(*indexer)\n\u001b[32m-> \u001b[39m\u001b[32m1342\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_chunk_getitems\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1343\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlchunk_coords\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1344\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlchunk_selection\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1345\u001b[39m \u001b[43m        \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1346\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlout_selection\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1347\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdrop_axes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdrop_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1348\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfields\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfields\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1349\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1350\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m out.shape:\n\u001b[32m   1351\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/np-ultra-psychedelics/.venv/lib/python3.12/site-packages/zarr/core.py:2183\u001b[39m, in \u001b[36mArray._chunk_getitems\u001b[39m\u001b[34m(self, lchunk_coords, lchunk_selection, out, lout_selection, drop_axes, fields)\u001b[39m\n\u001b[32m   2181\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m._meta_array, np.ndarray):\n\u001b[32m   2182\u001b[39m         contexts = ConstantMap(ckeys, constant=Context(meta_array=\u001b[38;5;28mself\u001b[39m._meta_array))\n\u001b[32m-> \u001b[39m\u001b[32m2183\u001b[39m     cdatas = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mchunk_store\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43mckeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontexts\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2185\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m ckey, chunk_select, out_select \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(ckeys, lchunk_selection, lout_selection):\n\u001b[32m   2186\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ckey \u001b[38;5;129;01min\u001b[39;00m cdatas:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/np-ultra-psychedelics/.venv/lib/python3.12/site-packages/zarr/_storage/store.py:179\u001b[39m, in \u001b[36mBaseStore.getitems\u001b[39m\u001b[34m(self, keys, contexts)\u001b[39m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgetitems\u001b[39m(\n\u001b[32m    155\u001b[39m     \u001b[38;5;28mself\u001b[39m, keys: Sequence[\u001b[38;5;28mstr\u001b[39m], *, contexts: Mapping[\u001b[38;5;28mstr\u001b[39m, Context]\n\u001b[32m    156\u001b[39m ) -> Mapping[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[32m    157\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Retrieve data from multiple keys.\u001b[39;00m\n\u001b[32m    158\u001b[39m \n\u001b[32m    159\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    177\u001b[39m \u001b[33;03m    keys and/or to utilize the contexts.\u001b[39;00m\n\u001b[32m    178\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {k: \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m keys \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/np-ultra-psychedelics/.venv/lib/python3.12/site-packages/zarr/storage.py:1118\u001b[39m, in \u001b[36mDirectoryStore.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   1116\u001b[39m filepath = os.path.join(\u001b[38;5;28mself\u001b[39m.path, key)\n\u001b[32m   1117\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m os.path.isfile(filepath):\n\u001b[32m-> \u001b[39m\u001b[32m1118\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fromfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1119\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1120\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/np-ultra-psychedelics/.venv/lib/python3.12/site-packages/zarr/storage.py:1092\u001b[39m, in \u001b[36mDirectoryStore._fromfile\u001b[39m\u001b[34m(fn)\u001b[39m\n\u001b[32m   1078\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m   1079\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_fromfile\u001b[39m(fn):\n\u001b[32m   1080\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Read data from a file\u001b[39;00m\n\u001b[32m   1081\u001b[39m \n\u001b[32m   1082\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1090\u001b[39m \u001b[33;03m    file reading logic.\u001b[39;00m\n\u001b[32m   1091\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1092\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m   1093\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m f.read()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import re \n",
    "\n",
    "session_folder = '/Volumes/aind/scratch/andrew.shelton/NPUltra_data/raw_npultra_data/2024-05-14_714527'\n",
    "session_name = filtered_session_list[0]\n",
    "session_pattern = r'\\d{4}-\\d{2}-\\d{2}_(\\d{6})'\n",
    "match = re.search(session_pattern, session_name)\n",
    "subject_id = match.group(1) \n",
    "nwb_path = [] \n",
    "\n",
    "# Search for directories containing the subject ID \n",
    "subject_dir = glob.glob(os.path.join(session_folder, f\"*{subject_id}*\"))   \n",
    "\n",
    "if not subject_dir:\n",
    "    print(f\"No directory containing '{subject_id}' found in {session_folder}\")\n",
    "\n",
    "nwb_pattern = os.path.join(subject_dir[0], \"*.nwb\")\n",
    "nwb_path.extend(glob.glob(nwb_pattern)) \n",
    "\n",
    "if not os.path.exists(nwb_path[0]):\n",
    "    print(f\"NWB file {nwb_path[0]} does not exist\")\n",
    "\n",
    "# Load nwb file using \n",
    "# \n",
    "with hdmf_zarr.NWBZarrIO(nwb_path[0], mode='r') as io:\n",
    "    nwbfile = io.read()\n",
    "    # Get units table \n",
    "    units_table = nwbfile.units.to_dataframe()\n",
    "\n",
    "# Modify units table \n",
    "modified_units_table = units_table.copy()  # Create a copy to avoid modifying the original\n",
    "modified_units_table = modified_units_table.rename(columns={\n",
    "    'ks_unit_id': 'unitID'\n",
    "})\n",
    "\n",
    "# Reorganize columns alphabetically \n",
    "modified_units_table = modified_units_table.reindex(sorted(modified_units_table.columns), axis=1)\n",
    "# Reorganize columns again by putting unitID first \n",
    "unitID_col = modified_units_table.pop('unitID') # Remove the column \n",
    "modified_units_table.insert(0, 'unitID', unitID_col) # Insert it back at the beginning \n",
    "\n",
    "# Save modified units table \n",
    "modified_units_table \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
