{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Core Loading Functions\n",
    "def load_units_table(session_folder):\n",
    "    \"\"\"Load units table from session folder.\"\"\"\n",
    "    units_table_path = os.path.join(session_folder, \"processed_data\", \"units\")\n",
    "    \n",
    "    if not os.path.exists(units_table_path):\n",
    "        logger.warning(f\"Units folder does not exist in {session_folder}\")\n",
    "        return None\n",
    "    \n",
    "    # Find filename for units table pickle (exclude units_epochs)\n",
    "    units_files = [f for f in os.listdir(units_table_path) \n",
    "                   if 'units_epoch' in f and 'units_epochs' not in f]\n",
    "    \n",
    "    if not units_files:\n",
    "        logger.warning(f\"No units table files found in {units_table_path}\")\n",
    "        return None\n",
    "    \n",
    "    units_table_file = os.path.join(units_table_path, units_files[0])\n",
    "    try:\n",
    "        units_table = pd.read_pickle(units_table_file)\n",
    "        logger.info(f\"Loaded units table from {units_table_file}\")\n",
    "        return units_table\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading units table from {units_table_file}: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_stimulus_table(session_folder):\n",
    "    \"\"\"Create stimulus table from opto_stim_df.csv and session metadata.\"\"\"\n",
    "    stim_path = os.path.join(session_folder, \"processed_data\", \"stim\")\n",
    "    \n",
    "    if not os.path.exists(stim_path):\n",
    "        logger.warning(f\"Stimulus folder does not exist in {session_folder}\")\n",
    "        return None\n",
    "    \n",
    "    # Find opto_stim_df file\n",
    "    opto_files = [f for f in os.listdir(stim_path) if 'opto_stim_df' in f]\n",
    "    \n",
    "    if not opto_files:\n",
    "        logger.warning(f\"No opto_stim_df files found in {stim_path}\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Load opto stimulus data\n",
    "        stim_table_path = os.path.join(stim_path, opto_files[0])\n",
    "        opto_stim_df = pd.read_csv(stim_table_path)\n",
    "        logger.info(f\"Loaded stimulus table from {stim_table_path}\")\n",
    "        \n",
    "        # Create new stimulus table with required columns\n",
    "        stim_table = pd.DataFrame()\n",
    "        stim_table['start_time'] = opto_stim_df['stim_on']\n",
    "        stim_table['stop_time'] = opto_stim_df['stim_off']\n",
    "        stim_table['stim_name'] = opto_stim_df['epoch_label']\n",
    "        stim_table['emission_location'] = opto_stim_df['probe']\n",
    "        \n",
    "        # Get metadata for wavelength and power\n",
    "        wavelength, power = load_stimulus_metadata(session_folder)\n",
    "        stim_table['wavelength'] = wavelength\n",
    "        stim_table['power'] = power\n",
    "        \n",
    "        return stim_table\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error creating stimulus table: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_stimulus_metadata(session_folder):\n",
    "    \"\"\"Extract wavelength and power from session metadata.\"\"\"\n",
    "    metadata_path = os.path.join(session_folder, \"metadata\", \"session.json\")\n",
    "    \n",
    "    if not os.path.exists(metadata_path):\n",
    "        logger.warning(f\"Metadata file does not exist: {metadata_path}\")\n",
    "        return None, None\n",
    "    \n",
    "    try:\n",
    "        with open(metadata_path, 'r') as f:\n",
    "            metadata = json.load(f)\n",
    "            \n",
    "        # Extract wavelength and power from stimulus_epochs with light_source_config\n",
    "        for epoch in metadata.get('stimulus_epochs', []):\n",
    "            light_config = epoch.get('light_source_config')\n",
    "            if light_config is not None:\n",
    "                wavelength = light_config.get('wavelength')\n",
    "                power = light_config.get('laser_power')\n",
    "                return wavelength, power\n",
    "                \n",
    "        return None, None\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading stimulus metadata: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def load_epoch_table(session_folder):\n",
    "    \"\"\"Create epoch table from stimulus CSV files and metadata.\"\"\"\n",
    "    stim_path = os.path.join(session_folder, \"processed_data\", \"stim\")\n",
    "    \n",
    "    if not os.path.exists(stim_path):\n",
    "        logger.warning(f\"Stimulus folder does not exist in {session_folder}\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Initialize lists to store epoch data\n",
    "        epoch_names = []\n",
    "        start_times = []\n",
    "        stop_times = []\n",
    "        \n",
    "        # Process each CSV file\n",
    "        stim_files = [f for f in os.listdir(stim_path) if f.endswith('.csv')]\n",
    "        if stim_files:\n",
    "            for stim_file in stim_files:\n",
    "                file_path = os.path.join(stim_path, stim_file)\n",
    "                \n",
    "                # Read required columns, handle missing stim_off\n",
    "                try:\n",
    "                    temp_df = pd.read_csv(file_path, usecols=['stim_on', 'stim_off', 'epoch_label'])\n",
    "                except ValueError:\n",
    "                    temp_df = pd.read_csv(file_path, usecols=['stim_on', 'epoch_label'])\n",
    "                    temp_df['stim_off'] = pd.NA\n",
    "                \n",
    "                # Process each unique epoch\n",
    "                for epoch_label in temp_df['epoch_label'].unique():\n",
    "                    epoch_data = temp_df[temp_df['epoch_label'] == epoch_label]\n",
    "                    \n",
    "                    # Determine stop_time based on epoch type\n",
    "                    if 'OptoTagging' in epoch_label:\n",
    "                        stop_time = epoch_data['stim_off'].max()\n",
    "                    else:\n",
    "                        stop_time = epoch_data['stim_on'].max()\n",
    "                    \n",
    "                    start_time = epoch_data['stim_on'].min()\n",
    "                    \n",
    "                    epoch_names.append(epoch_label)\n",
    "                    start_times.append(start_time)\n",
    "                    stop_times.append(stop_time)\n",
    "\n",
    "        # Create initial epoch table\n",
    "        epoch_table = pd.DataFrame({\n",
    "            'stim_name': epoch_names,\n",
    "            'start_time': start_times,\n",
    "            'stop_time': stop_times\n",
    "        })\n",
    "        \n",
    "        # Sort by start_time\n",
    "        epoch_table = epoch_table.sort_values(by='start_time').reset_index(drop=True)\n",
    "        \n",
    "        # Add injection epochs\n",
    "        epoch_table = add_injection_epochs(epoch_table, session_folder)\n",
    "        \n",
    "        logger.info(f\"Created epoch table with {len(epoch_table)} epochs\")\n",
    "        return epoch_table\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error creating epoch table: {e}\")\n",
    "        return None\n",
    "\n",
    "def add_injection_epochs(epoch_table, session_folder):\n",
    "    \"\"\"Add injection epochs between OptoTagging_0 and Spontaneous_1.\"\"\"\n",
    "    metadata_path = os.path.join(session_folder, \"metadata\", \"session.json\")\n",
    "    \n",
    "    if not os.path.exists(metadata_path):\n",
    "        return epoch_table\n",
    "    \n",
    "    try:\n",
    "        with open(metadata_path, 'r') as f:\n",
    "            metadata = json.load(f)\n",
    "\n",
    "        # Look for injection notes in OptoTagging epochs\n",
    "        for epoch in metadata.get('stimulus_epochs', []):\n",
    "            if epoch.get('stimulus_name') == 'OptoTagging':\n",
    "                notes = epoch.get('notes', '')\n",
    "                \n",
    "                if 'Saline injection' in notes or 'Psilocybin injection' in notes:\n",
    "                    # Determine injection type\n",
    "                    injection_name = 'Saline_Inj' if 'Saline' in notes else 'Psilocybin_Inj'\n",
    "                    \n",
    "                    # Find OptoTagging_0 stop and Spontaneous_1 start times\n",
    "                    opto_0_stop = None\n",
    "                    spont_1_start = None\n",
    "                    \n",
    "                    for idx, row in epoch_table.iterrows():\n",
    "                        if row['stim_name'] == 'OptoTagging_0':\n",
    "                            opto_0_stop = row['stop_time']\n",
    "                        elif row['stim_name'] == 'Spontaneous_1':\n",
    "                            spont_1_start = row['start_time']\n",
    "                    \n",
    "                    if opto_0_stop is not None and spont_1_start is not None:\n",
    "                        injection_start = opto_0_stop + 0.01\n",
    "                        injection_end = spont_1_start - 0.01\n",
    "                        \n",
    "                        # Find insertion point\n",
    "                        insert_idx = None\n",
    "                        for idx, row in epoch_table.iterrows():\n",
    "                            if row['stim_name'] == 'OptoTagging_0':\n",
    "                                insert_idx = idx + 1\n",
    "                                break\n",
    "                        \n",
    "                        # Insert injection epoch\n",
    "                        if insert_idx is not None:\n",
    "                            new_row = pd.DataFrame({\n",
    "                                'stim_name': [injection_name],\n",
    "                                'start_time': [injection_start],\n",
    "                                'stop_time': [injection_end]\n",
    "                            })\n",
    "                            \n",
    "                            epoch_table = pd.concat([\n",
    "                                epoch_table.iloc[:insert_idx],\n",
    "                                new_row,\n",
    "                                epoch_table.iloc[insert_idx:]\n",
    "                            ], ignore_index=True)\n",
    "                            \n",
    "                            logger.info(f\"Added {injection_name} epoch from {injection_start} to {injection_end}\")\n",
    "                    \n",
    "                    break  # Only process first OptoTagging epoch with injection note\n",
    "                    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error adding injection epochs: {e}\")\n",
    "    \n",
    "    return epoch_table\n",
    "\n",
    "# Table Processing Functions\n",
    "def prepare_analysis_table(units_table):\n",
    "    \"\"\"Prepare units table for analysis storage.\"\"\"\n",
    "    if units_table is None:\n",
    "        return None\n",
    "    \n",
    "    analysis_table = units_table.copy()\n",
    "    \n",
    "    # Remove probe_config column if it exists\n",
    "    if 'probe_config' in analysis_table.columns:\n",
    "        analysis_table = analysis_table.drop('probe_config', axis=1)\n",
    "        logger.info(\"Removed probe_config column from analysis table\")\n",
    "    \n",
    "    return analysis_table\n",
    "\n",
    "def prepare_stimulus_table(stimulus_table):\n",
    "    \"\"\"Prepare stimulus table for analysis storage.\"\"\"\n",
    "    return stimulus_table.copy() if stimulus_table is not None else None\n",
    "\n",
    "def prepare_epoch_table(epoch_table):\n",
    "    \"\"\"Prepare epoch table for analysis storage.\"\"\"\n",
    "    return epoch_table.copy() if epoch_table is not None else None\n",
    "\n",
    "# Session Processing Functions\n",
    "def process_session_tables(session, base_path):\n",
    "    \"\"\"Process all tables for a single session.\"\"\"\n",
    "    session_folder = os.path.join(base_path, session)\n",
    "    \n",
    "    if not os.path.exists(session_folder):\n",
    "        logger.error(f\"Session folder {session_folder} does not exist\")\n",
    "        return None\n",
    "    \n",
    "    # Load all tables\n",
    "    units_table = load_units_table(session_folder)\n",
    "    stimulus_table = load_stimulus_table(session_folder)\n",
    "    epoch_table = load_epoch_table(session_folder)\n",
    "    \n",
    "    # Prepare tables for analysis\n",
    "    analysis_table = prepare_analysis_table(units_table)\n",
    "    prepared_stimulus = prepare_stimulus_table(stimulus_table)\n",
    "    prepared_epochs = prepare_epoch_table(epoch_table)\n",
    "    \n",
    "    session_data = {\n",
    "        'session': session,\n",
    "        'analysis_table': analysis_table,\n",
    "        'stimulus_table': prepared_stimulus,\n",
    "        'epoch_table': prepared_epochs\n",
    "    }\n",
    "    \n",
    "    logger.info(f\"Processed tables for session: {session}\")\n",
    "    return session_data\n",
    "\n",
    "def process_multiple_sessions(session_list, base_path):\n",
    "    \"\"\"Process tables for multiple sessions.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for session in session_list:\n",
    "        logger.info(f\"Processing session: {session}\")\n",
    "        session_data = process_session_tables(session, base_path)\n",
    "        \n",
    "        if session_data:\n",
    "            results.append(session_data)\n",
    "        else:\n",
    "            logger.warning(f\"Failed to process session: {session}\")\n",
    "    \n",
    "    logger.info(f\"Successfully processed {len(results)} out of {len(session_list)} sessions\")\n",
    "    return results\n",
    "\n",
    "# Example usage with your existing data\n",
    "def run_analysis_pipeline(filtered_sessions, base_path=\"/Volumes/scratch/andrew.shelton/NPUltra_data/raw_npultra_data/\"):\n",
    "    \"\"\"Run the complete analysis pipeline using your filtered_sessions DataFrame.\"\"\"\n",
    "    session_list = filtered_sessions['session'].tolist()\n",
    "    return process_multiple_sessions(session_list, base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing session: 2024-05-14_714527\n",
      "INFO:__main__:Loaded units table from /Volumes/scratch/andrew.shelton/NPUltra_data/raw_npultra_data/2024-05-14_714527/processed_data/units/2024-05-14_714527_1_units_epoch.pkl\n",
      "INFO:__main__:Loaded stimulus table from /Volumes/scratch/andrew.shelton/NPUltra_data/raw_npultra_data/2024-05-14_714527/processed_data/stim/opto_stim_df.csv\n",
      "INFO:__main__:Added Saline_Inj epoch from 2714.53929 to 3058.6834599999997\n",
      "INFO:__main__:Created epoch table with 13 epochs\n",
      "INFO:__main__:Removed probe_config column from analysis table\n",
      "INFO:__main__:Processed tables for session: 2024-05-14_714527\n",
      "INFO:__main__:Successfully processed 1 out of 1 sessions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session: 2024-05-14_714527\n",
      "Analysis table shape: (350, 53)\n",
      "Stimulus table shape: (600, 6)\n",
      "Epoch table shape: (13, 3)\n"
     ]
    }
   ],
   "source": [
    "# Your existing workflow becomes:\n",
    "base_path = \"/Volumes/scratch/andrew.shelton/NPUltra_data/raw_npultra_data/\"\n",
    "\n",
    "# Import recording session summary excel spreadsheet \n",
    "recording_summary = \"/Volumes/scratch/andrew.shelton/NPUltra_data/raw_npultra_data/NPUltra_recording_summary.xlsx\"\n",
    "recording_summary_table = pd.read_excel(recording_summary)\n",
    "\n",
    "# Filter table for sessions of interest (experiment = NPUltra_psychedelics, uploaded to CO = yes)\n",
    "filtered_sessions = recording_summary_table[\n",
    "    (recording_summary_table['experiment'] == 'NPUltra_psychedelics') &\n",
    "    (recording_summary_table['uploaded to CO'] == 'yes')]\n",
    "\n",
    "filtered_sessions\n",
    "\n",
    "# # Process all sessions\n",
    "# results = run_analysis_pipeline(filtered_sessions, base_path)\n",
    "\n",
    "# # Or process a subset (like your [0:1] slice)\n",
    "session_subset = filtered_sessions['session'].tolist()[0:1]\n",
    "results = process_multiple_sessions(session_subset, base_path)\n",
    "\n",
    "# Access individual tables\n",
    "for session_data in results:\n",
    "    print(f\"Session: {session_data['session']}\")\n",
    "    if session_data['analysis_table'] is not None:\n",
    "        print(f\"Analysis table shape: {session_data['analysis_table'].shape}\")\n",
    "    if session_data['stimulus_table'] is not None:\n",
    "        print(f\"Stimulus table shape: {session_data['stimulus_table'].shape}\")\n",
    "    if session_data['epoch_table'] is not None:\n",
    "        print(f\"Epoch table shape: {session_data['epoch_table'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
